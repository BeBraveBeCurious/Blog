# 强化学习入门

## Day1入门

### 1. What

基于环境而行动，以取得最大化的预期利益。

智能体`agent`在环境`environment`中学习，根据环境的状态`state`（或观测到的`observation`），执行动作`action`，并根据环境的反馈 `reward`（奖励）来指导更好的动作

- environment, 环境，i.e., 赛车和赛道。
  - 环境信息>>观测信息，天空的颜色>>前方的赛道
- agent, 智能体：i.e., 一辆赛车的控制程序
  - 赛车的传感器，速度、方向、位置传感器 -- 接受环境的信息**(observation）**
  - 赛车根据环境信息调整方向、速度**(action)**的行为：加速减速匀速
  - 赛车根据环境信息调整方向、速度(action)来控制赛车状态**(state)**
- Transit
  - 做出一个新的action，可能转向一个新的state：$老的state + action\stackrel{transit}{\longrightarrow}新state$
  - $$T(s'|s, a) = P(S_{t+1} = s' | S_t = s, A_t = a)$$
- policy
  - deterministic  $\pi(s) = a$
  - stochastic从一个distribution里面抽取 -> 同样的老state和同样action并不一定是同样的新state
  - stochastic   $\pi(a | s) = P_\pi[A = a | S = s]$
- reward
  - **reward**, 评判标准。 $R(s, a, s') = R_{t+1} = Reward(oldstate, action, newstate)$

```
老state(v = 1) -> action(加速) -> 新state(v = 2) -> reward = 1
```

![Observation\Action\Reward](https://i.loli.net/2020/06/24/MLpGbI7cN9AOBVo.png)





### 2. RL和SL的区别

```
input(i.e. picture) -> model -> output(continuous==regression, discrete==classification) 模型构建输入输出的关系 --focus 认知问题
```

``` 
unsupervised learning: 聚类-探索数据之间的隐藏关系
```

```
start -> state1 -> action -> state2 -> action -> ... -> end
state1 -> state2 -> ... -> staten 中间可能会回到之前的state，不是完全链式的结构
```

RL处理决策问题

#### difference

- states in RL are discrete 
- SL的求导、梯度下降不适合更新

#### same

action 是自己定义的，可以通过不同的方式计算

DQN == Deep neural network + Q-table

DQN使用CNN读取游戏输入 + 含有ReLU的Linear Layer来输出actions



### 3. How解决问题

- 不断试错探索action、吸取经验和教训reward、持续不断的优化策略policy->选择action,从环境中获得更好的反馈：更高的reward
- 2种学习方案：value-based + policy-based

### 4. in and where 算法和环境

- 经典算法：`Q-learning`、`Sarsa`、`DQN`、`Policy Gradient`、`A3C`、`DDPG`、`PPO`
- 环境分类：离散控制场景（输出动作可数==Q-table）、连续控制场景（输出动作值不可数==DDPG 方向盘角度）
- 经典环境库`GYM`将环境交互接口规范化为：重置环境`reset()`、交互`step()`、渲染`render()`
- 强化学习框架库`PARL`抽象为`Model`、`Algorithm`、`Agent`三层

## day2 MDP\SARSA\Q-learning

### 1. Markov Decision Process





### 2. Sarsa

- state- action-reward-state'-action'

- 和环境交互不断更新的`reward`存在Q表格：以`state`为行，`action`为列【上下左右停】

  $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$$

  - $R_{t+1} - \gamma Q(S_{t+1}, A_{t+1})$ 目标值， 当前值$Q(S_t, A_t)$

  - 软更新 $\alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]$

- `Sarsa`在训练中为了更好的探索环境，采用`ε-greedy`方式来训练，有一定概率随机选择动作输出
- Exploration 新饭店探索新口味 & exploitation 旧饭店选好吃的



### 3. Q-learning

- `Q-learning`也是采用`Q`表格的方式存储`Q`值（状态动作价值），决策部分与`Sarsa`是一样的，采用`ε-greedy`方式增加探索。

- `Q-learning`的更新公式为：

  $$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha[R_{t+1} + \gamma \mathop{max}\limits_{\alpha}Q(S_{t+1}, a) - Q(S_t, A_t)]$$

- `Q-learning`跟`Sarsa`不一样的地方是更新`Q`表格的方式。

  - `Sarsa`是`on-policy`的更新方式，**先做出动作再更新**。

  - `Q-learning`是`off-policy`的更新方式，更新`learn()`时无需获取下一步实际做出的动作`next_action`，并**假设下一步动作是取最大`Q`值的动作**。

    ![off-policy](https://i.loli.net/2020/06/24/cjgo9ts48TxFapM.png)

- 



### 4. Sarsa vs. Q-learning

![Sarsa_Qlearning](https://i.loli.net/2020/06/24/njSEte4yUH89Voi.png)

![image-20200623235155700](https://i.loli.net/2020/06/24/R8CY7L3uWveGFJn.png)



### 5.表格方法总结

![基于表格型方法求解RL](https://i.loli.net/2020/06/24/gwb7iOzIW5mHxQA.png)







### 神经网络方法总结



![基于神经网络的RL](https://i.loli.net/2020/06/24/nLgpslQ37Nr9jZe.png)







### 策略梯度方法总结



![基于策略梯度求解的RL](https://i.loli.net/2020/06/24/gu7kIWlnPrpS6sR.png)







### 连续动作空间RL总结



![连续动作空间求解RL](https://i.loli.net/2020/06/24/YIZBCwOcKdv45tL.png)